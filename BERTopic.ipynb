{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3rWwNKIWAb5"
   },
   "source": [
    "# Bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGBNX0vNOq2Y",
    "outputId": "c786c5fd-f738-4f03-ea36-c8454320a3e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Using cached bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from bertopic) (1.21.5)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Using cached umap_learn-0.5.3-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from bertopic) (1.0.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from bertopic) (4.64.1)\n",
      "Collecting hdbscan>=0.8.29\n",
      "  Using cached hdbscan-0.8.30.tar.gz (5.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from bertopic) (5.9.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from bertopic) (1.4.4)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (0.29.32)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.9.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.2.0)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.15.2-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "Collecting torch>=1.6.0\n",
      "  Using cached torch-2.0.1-cp39-cp39-win_amd64.whl (172.4 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tyler\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.10-py3-none-any.whl\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\tyler\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (23.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tyler\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tyler\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\tyler\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.38.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (63.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tyler\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.8.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.7.9)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.3.1-cp39-cp39-win_amd64.whl (263 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: click in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.0.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tyler\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.2.1)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (pyproject.toml): started\n",
      "  Building wheel for hdbscan (pyproject.toml): finished with status 'error'\n",
      "Failed to build hdbscan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for hdbscan (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [40 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\flat.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\plots.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\prediction.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\validity.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\__init__.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  creating build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_prediction_utils.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  running build_ext\n",
      "  cythoning hdbscan/_hdbscan_tree.pyx to hdbscan\\_hdbscan_tree.c\n",
      "  C:\\Users\\tyler\\AppData\\Local\\Temp\\pip-build-env-874rje2x\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\tyler\\AppData\\Local\\Temp\\pip-install-czr6mate\\hdbscan_57017c14fca944d88f968e59c6c9d389\\hdbscan\\_hdbscan_tree.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_hdbscan_linkage.pyx to hdbscan\\_hdbscan_linkage.c\n",
      "  C:\\Users\\tyler\\AppData\\Local\\Temp\\pip-build-env-874rje2x\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\tyler\\AppData\\Local\\Temp\\pip-install-czr6mate\\hdbscan_57017c14fca944d88f968e59c6c9d389\\hdbscan\\_hdbscan_linkage.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_hdbscan_boruvka.pyx to hdbscan\\_hdbscan_boruvka.c\n",
      "  C:\\Users\\tyler\\AppData\\Local\\Temp\\pip-build-env-874rje2x\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\tyler\\AppData\\Local\\Temp\\pip-install-czr6mate\\hdbscan_57017c14fca944d88f968e59c6c9d389\\hdbscan\\_hdbscan_boruvka.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_hdbscan_reachability.pyx to hdbscan\\_hdbscan_reachability.c\n",
      "  C:\\Users\\tyler\\AppData\\Local\\Temp\\pip-build-env-874rje2x\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\tyler\\AppData\\Local\\Temp\\pip-install-czr6mate\\hdbscan_57017c14fca944d88f968e59c6c9d389\\hdbscan\\_hdbscan_reachability.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_prediction_utils.pyx to hdbscan\\_prediction_utils.c\n",
      "  C:\\Users\\tyler\\AppData\\Local\\Temp\\pip-build-env-874rje2x\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\tyler\\AppData\\Local\\Temp\\pip-install-czr6mate\\hdbscan_57017c14fca944d88f968e59c6c9d389\\hdbscan\\_prediction_utils.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/dist_metrics.pyx to hdbscan\\dist_metrics.c\n",
      "  C:\\Users\\tyler\\AppData\\Local\\Temp\\pip-build-env-874rje2x\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\tyler\\AppData\\Local\\Temp\\pip-install-czr6mate\\hdbscan_57017c14fca944d88f968e59c6c9d389\\hdbscan\\dist_metrics.pxd\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  building 'hdbscan._hdbscan_tree' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hdbscan\n",
      "ERROR: Could not build wheels for hdbscan, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JUPdTqeV59H"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMqopTHIPVy1",
    "outputId": "ea356581-5746-42c0-9876-e020d13f8229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title          Query\n",
      "0                         South America  Google News  South America\n",
      "1  South American trade bloc Mercosur holds summi...  South America\n",
      "2  Minnesota National Guard Deploying to South Am...  South America\n",
      "3  Extreme weather in Latin America unlocks vicio...  South America\n",
      "4  Volkswagen aims to grow 40 in S America throug...  South America\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/titles_data.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7paBSAOlWNJu"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYwxfAg4WPe7",
    "outputId": "53c15c93-34b6-49f6-c6c5-e7ebe314a024"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess_title(title):\n",
    "    title = title.lower()\n",
    "    tokens = word_tokenize(title)\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    preprocessed_title = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l15843pMWhpS",
    "outputId": "938a569e-c952-4617-a411-40e9b8f53e45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                               south america google news\n",
      "1       south american trade bloc mercosur holds summi...\n",
      "2       minnesota national guard deploying south ameri...\n",
      "3       extreme weather latin america unlocks vicious ...\n",
      "4       volkswagen aims grow 40 america ev subscriptio...\n",
      "                              ...                        \n",
      "1214    us trying mend ties venezuela one big reason o...\n",
      "1215    decade maduro migration marks venezuelans live...\n",
      "1216    venezuelas juan guaid√≥ seeks support washingto...\n",
      "1217    joint statement venezuela negotiations united ...\n",
      "1218    opinion venezuelas crisis must resolved peacef...\n",
      "Name: Preprocessed Title, Length: 1219, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['Preprocessed Title'] = df['Title'].apply(preprocess_title)\n",
    "print(df['Preprocessed Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pxy7S-8uWmkb"
   },
   "source": [
    "Bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0ONKh9mXWmEt",
    "outputId": "76022c54-de56-47ff-f15d-b1586bad5976"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BERTopic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19408\\3683223399.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Preprocessed Title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mn_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BERTopic' is not defined"
     ]
    }
   ],
   "source": [
    "model = BERTopic()\n",
    "topics, _ = model.fit_transform(df['Preprocessed Title'])\n",
    "\n",
    "n_topics = 5\n",
    "\n",
    "print(model.get_topics(0,1,2))\n",
    "\n",
    "model.get_topic_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
